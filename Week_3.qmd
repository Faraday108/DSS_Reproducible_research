---
title: "Week 3"
format: html
editor: visual
---

# Video: Communicating Results  
You cannot present everything at once, so what hierarcy of information should you follow?  
* People are busy, especially managers or leaders
* Results of data analysis are sometimes in oral form, but often first presented via email. 
* It is often useful to break the results of an analysis into different levels of granularity / detail
* How to get responses from busy people: <http://goo.gl/sJDb9V>  

## Hierarchy of information: Research paper  
* Title / author list
* Abstract
* Body / Results
* Supplemental materials / the gory details
* Code / data/ really gory details

## Hierarchy of information: Email presentation  
* Subject line / sender info 
  * At a minimum, include one
  * Can you summarize findings in one sentence? 
* Email body
  * A brief descripton of the problem / context; recall what was proposed and executed; summarize findings / results; 1-2 paragraphs
  * If action needs to be taken, suggest some options and make them concrete
  * If questions need to be addressed, try to make them yes / no
* Attachments
  * R markdown file
  * knitr report
  * Stay concise; don't spit out pages of code
* Links to supplementary materials
  * Code / software / data
  * Github repository / project web site

# Video: RPubs  
Service R Studio provides; once you create an account you can publish R Markdown documents here. 

# Video: Reproducible Research Checklist  
By no means a comprehensive checklist but is a place to start.  

## DO: Start with good science  
* Garbage in, garbage out
* Coherent, focused question simplifies many problems
* Working with good collaborators reinforces good practices
* Something that's interesting to you will (hopefully) motivate good habits  

## DON'T: Do things by hand  
* Editing spreadsheets of data to "clean it up"
  * Removing outliers
  * QA/QC
  * Validating
* Editing tables or figures (rounding/formatting)
* Downloading data round your computer; splitting/reformatting data files
* "We're just going to do this once..."

Things done by hand need to be precisely documented (harder than it sounds!). By doing this programatically, you create a reproducible pathway others can follow. 

## DON'T: Poing and Click  
* Many data processing / statistical packages have GUI's. 
* GUI's are convenient / intuitive but actions that you take with a GUI can be difficult for others to reproduce
* Some GUIs produce a log file or script which includes equivalent commands; these can be saved for later examination
* In general, be careful with data analysis software that is highly *interactive*; ease of use can sometimes lead to non-reproducible analyses
* Other interactive software such as text editors are usually fine  

## DO: Teach a computer  
* If something needs to be done as part of your analysis, try to teach your computer to do it (even if you only need to do it once)
* In order to give your computer instructions, you need to write down exactly what you need to do and how it should be done
* Teaching a computer almost guarantees reproducibility

For example, by hand, you can:

1. Go to the UCI Machine learning Repository at <http://archive.ics/uci/edu/ml/>
2. Down the bike sharing dataset by clicking on the data folder, then clicking on the link to the zip file of dataset, and choosing "Save linked file as..." and then saving it to a folder on your computer. 

OR, you can teach your computer to do the same thing using R: 

```{r, eval = FALSE}
download.file("http://archive.ics.uci.edu/machine-learning-databases/00275/Bike-Sharing-Dataset.zip", "ProjectData/Bike-Sharing-Dataset.zip")
```

Notice that:  
* The full URL to the dataset is specified (no clicking through a series of links)
* The name of the file saved to your computer is specified
* THe directory in which the file was saved is specified ("ProjectData")
* Code can always be executed in R (as long as the link is available)

## DO: Use some version control  
* Slows things down
* Add changes in small chunks (don't just do one massive commit)
* Track/tag snapshots, revert to old versions
* Software like GitHub/GitBucket/SourceForge make it easy to publish results. 

Main feature is it helps slow you down a bit. You *need* to track what is changed. 

## DO: Keep track of your software environment  
* If you work on a complex project with many tools/datasets, the software and computing environment can be critical for reproducing your analysis
* **Computer architecture**: CPU, GPU (not as important today)
* **Operating system**: Windows, Mac, Linux
* **Software toolchain**: Compilers, interpreters, command shell, programming language, database backends, data analysis software
* **Supporting software/infrastructure**: Libraries, R Packages, dependencies
* **External Dependencies**: Web sites, data repositories, remote databases, software repositories
* **Version numbers**: Ideally for everything if available

In R, use `sessionInfo()` to find version. 

## DON'T: Save Output  
* Avoid saving data analysis output (tables, figures, summaries, processed data, etc) except perhaps temporarily for efficiency purposes
* If a stray output file cannot be easily connected with the means by which it is created, then it is not reproducible
* Save the data + code that generated the output, rather than the output itself
* Intermediate files are okay as long as there is clear documentation of how they were created. 

## DO: Set your seed  
* Random number generators generate pseudo-random numbers based on an initial seed (usually a number or a set of numbers)
  * In R, use `set.seed()`
* Setting the seed allows for the stream of random numbers to be exactly reproducible
* Whenever you generate random numbers for a non-trivial purpose, **always set the seed**

## DO: Think about the entire pipeline  
* Data analysis is a lengthy process; it is not just tables/figures/reports
* Raw data to processed data to analysis to report
* How you got to the end is just as important as the end itself
* The more of the data analysis pipeline you can make reproducible, the better. 

## Summary: Checklist  
* Are we doing good science? 
* Was any part of this analysis done by hand? 
  * If so, are those parts *precisely* documented
  * Does the documentation match reality
* Have we taught a computer to do as much as possible (ie coded)? 
* Are we using a VC system?
* Have we documented our software environment? 
* Have we saved any output that cannot be reconstructed? 
* How far back in the analysis pipeline can we go before our results are no longer reproducible? 

# Video: Evidence-based data analysis  
Replication and Reproducibility  
Replication  
* Focuses on the validity of the scientific claim 
* "Is this claim true?"
* The ultimate standard for strengthening scientific evidence 
* New investigators, data, analytical methods, laboratories, instruments, etc
* Particularly important in studies that can impact broad policy or regulation decisions

Reproducibility  
* Focuses on the validity of the data analysis
* "Can we trust this analysis?"
* Arguably a minimum standard for any scientific study
* New investigators, same data, same methods
* Important when replication is impossible 

## Results of replication difficulty  
* Even basic analyses are difficult to describe
* Heavy computational requirements are thrust upon people without adequate training in statistics and computing
* Errors are more easily introduced into long analysis pipelines
* Knowledge transfer is inhibited
* Results are difficult to replicate or reproduce
* Complicated analyses can't be trusted

## What problem does RR solve?  
* Transparency
* Data availability
* Software / methods available
* Improved transfer of knowledge
* We do **NOT** get validity or correctness of analysis

An analysis can be reproducible and still be wrong. We want to know "can we trust this analysis?"

## Problems with RR  
The premise is that with data/code available, people can check each other and the whole system is self-correcting  
* Addresses the most "downstream" aspect of the research process - post publication
* Assumes everyone plays by the same rules and wants to achieve the same goals (scientific discovery)
* We would like something that can happen further upstream 

## Who reproduces research?  
* For reproducibility to be effective, someone needs to do something. 
  * Rerun the analysis, check results match
  * Check for code bugs
  * Try alternate approaches, check sensitivity
* The need for someone to do something is inherited from traditional notions of replication
* Who is "someone" and what are their goals? 
* Original investigators, general public, scientists, people who want disprove. 

## The story so far  
* RR brings transparency (code + data) and increased transfer of knowledge
* A lot of discussion on how to get people to share data
* Key question of "is this trustworthy" is not addressed by reproducibility
* RR addresses potential problems *after* they occurred (downstream)
* Secondary analyses are inevitably colored by the interest/motivations of others

## Evidence based data analysis  
* Most data analyses involve stringing together many different tools and methods
* Some methods may be standard for a given field, but others are applied ad hoc
* We should apply thoroughly studied (via stat research), mutually agreed upon methods to analyze data whenever possible
* There should be evidence to justify the application of a given method. 

## Evidence based data analysis  
* Create analytic pipelines from evidence-based components - standardize it
* A deterministic statistical machine 
* Once an evidence based-analytic pipeline is established, we shouldn't mess with it
* Analysis with a "transparent box"
* Reduce the "researcher degrees of freedom"
* Analogous to a pre-specified clinical trial protocol

## A curated library of data analysis  
* Provide packages that encode data analysis pipelines for given problems, technologies, questions 
* Curated by experts knowledgeable in the field
* Documentation/references given supporting each module in the pipeline
* Changes introduced after passing relevant benchmarks/unit tests. 

